{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783184b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f87a45b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordfreq import word_frequency, top_n_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfe13b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Het beeldhouwwerk is gemaakt in opdracht van Johan Maurits voor de toenmalige overtuin van het Mauritshuis in Den Haag en bevond zich aan het einde van de zichtas van deze tuin.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d104d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_19652/1721810298.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0mtop_list_string\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m' '\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtop_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mfreqwords\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprocess_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtop_list_string\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m#  if not (token.is_stop or token.is_punct or token.is_space)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[0msamplewords\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprocess_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msample_text\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[0mbasic_word_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'sample_text' is not defined"
     ]
    }
   ],
   "source": [
    "def process_text(text):\n",
    "    return [token.lemma_ for token in nlp(text) if not (token.is_punct or token.is_space)]\n",
    "\n",
    "\n",
    "top_list = top_n_list('nl', 20000)\n",
    "top_list_string = ' '.join(top_list)\n",
    "freqwords = process_text(top_list_string)  #  if not (token.is_stop or token.is_punct or token.is_space)\n",
    "samplewords = process_text(sample_text)\n",
    "\n",
    "basic_word_count = 0\n",
    "for word in tqdm(samplewords, total=(len(samplewords))):\n",
    "    if word in freqwords:\n",
    "        basic_word_count += 1\n",
    "    else:\n",
    "        print(word)\n",
    "basic_word_count\n",
    "\n",
    "# words\n",
    "# for i, word in enumerate(top_list):\n",
    "#     if word != words[i]:\n",
    "#         print(f'{words[i]} - {word}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f653e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_text = '''\n",
    "Ja goedenavond. Het is vandaag 1 september. En zoals u ongetwijfeld nog weet: de laatste formele datum op onze routekaart van coronamaatregelen. Dat is de directe aanleiding voor deze persconferentie. Ik zeg maar meteen: we hebben geen groot of onverwacht nieuws vandaag. Toch denken we dat het goed is de stand van zaken met u te bespreken, want er is natuurlijk na de laatste persconferentie van twee weken geleden wel het nodige gebeurd.\n",
    "We willen vandaag twee vragen met u doornemen. Ten eerste de duiding: waar staan we nu, ongeveer een half jaar na het begin van de uitbraak? Hoe beoordelen we de recente cijfers? Daar zal ik kort iets over zeggen. Ten tweede, wat doen we nu en de komende tijd om die tweede golf voor te blijven en er toch op voorbereid te zijn? Daar zal Hugo de Jonge zo meteen op ingaan.\n",
    "Maar voordat ik de cijfers induik, wil ik om te beginnen een keer duidelijk gezegd hebben dat uiteraard iedereen vragen mag stellen bij wat we als kabinet doen en daar ook kritiek op mag hebben. Sterker nog: het zou gek zijn als dat niet zou gebeuren. Het is volkomen logisch dat mensen na een half jaar van beperkingen zelf op onderzoek uitgaan. Of dat er mensen zijn die vanuit hun eigen discipline iets vinden over wat virologen en medici ons adviseren en wat wij als kabinet besluiten. Dat brengt ons verder en daar staan we dus open voor. Ik zeg er wel eerlijk bij: we kunnen bij onze beslissingen niet om de cijfers heen en we zullen ook heel precies moeten blijven kijken hoe het virus zich gedraagt. Maar dat wil niet zeggen dat er maar één waarheid is over de beste manier om met de gevolgen van corona om te gaan. Daarom praten we ook buiten het OMT met deskundigen. En daar zal Hugo de Jonge zo nog meer over zeggen. Ik had vanmiddag op het Catshuis een vierde gesprek met jongeren, dit keer samen met Eric Wiebes en Kajsa Ollongren. Dat helpt ons echt. En dus gaan Hugo de Jonge en ik volgende week een online sessie organiseren om van zoveel mogelijk mensen goede ideeën en kritische vragen te horen.\n",
    "Die manier van werken past bij wat we steeds hebben gezegd. We werken op basis van voortschrijdend inzicht. Niet met 100% van de kennis, maar wel met steeds meer kennis. We leren per dag, per week, per maand en we stellen het beleid daarop bij. Zo namen we in maart bijvoorbeeld op gezag van de Wereldgezondheidsorganisatie aan dat mensen gemiddeld 2 weken op de IC zouden liggen. Dat bleken er bijna 3 te zijn. Een ander voorbeeld: 14 dagen quarantaine bleek gaandeweg naar 10 dagen te kunnen. Of denk aan de routekaart zelf, die in het voorjaar steeds door ons eigen goede gedrag werd ingehaald, waardoor we voor de zomer ineens grote stappen konden zetten. Of aan twee weken geleden, toen juist bleek dat we weer even op de rem moesten trappen.\n",
    "We zagen toen de besmettingscijfers nog flink oplopen, vooral in de privésfeer. En dus hebben we gezegd: ontvang nou niet meer dan 6 mensen thuis, registreer je gasten in de horeca en werk ook na 1 september zoveel mogelijk thuis. Nu zien we dat de besmettingscijfers weer stabiel zijn, of zelfs licht dalen. We zitten nu op een plateau van rond de 500 besmettingen per dag en het aantal ziekenhuisopnamen en ic-patiënten is ook stabiel op een niveau dat de zorg aankan. Dat beroemde R-getal dat zegt hoeveel andere mensen iedere besmette persoon aansteekt, zat vandaag voor het eerst sinds een tijdje weer net onder de 1.\n",
    "'''\n",
    "print(len(process_text(sample_text)))\n",
    "Counter(samplewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_list[-1]\n",
    "print(word_frequency('decoraties', 'nl'))\n",
    "print(word_frequency(top_list[-100], 'nl'))\n",
    "word_frequency('aansteken', 'nl')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "----------------------------------\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f6d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_conference_data(conference_data: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Preprocesses the text of a single conference\n",
    "\n",
    "    :return: a tuple containing Rutte texts and De Jonge texts respectively per press conference\n",
    "    \"\"\"\n",
    "\n",
    "    # Starts with saving text for Rutte\n",
    "    save_text = 'rutte'\n",
    "    words = set()\n",
    "\n",
    "    # Only keep the sentences of Rutte and De Jonge\n",
    "    text_rutte, text_de_jonge = [], []\n",
    "    for line in conference_data:\n",
    "        if save_text == 'rutte':\n",
    "            text_rutte.append(line)\n",
    "        if save_text == 'de jonge':\n",
    "            text_de_jonge.append(line)\n",
    "\n",
    "        if line.isupper():\n",
    "            if 'RUT' in line:\n",
    "                save_text = 'rutte'\n",
    "            elif 'DE JONGE' in line:\n",
    "                save_text = 'de jonge'\n",
    "            else:\n",
    "                words.add(line)\n",
    "                save_text = 'other'\n",
    "\n",
    "    # get rid of newline characters\n",
    "    speakers_text = [text_rutte, text_de_jonge]\n",
    "\n",
    "    for i, data in enumerate(speakers_text):\n",
    "        speakers_text[i] = ''.join(data).replace(\"\\n\", \" \")\n",
    "\n",
    "    return tuple(speakers_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_tfidf(text_by_speaker, create_csv) -> tuple:\n",
    "    \"\"\"\n",
    "\n",
    "    Calculates the tfidf per speaker per conference\n",
    "\n",
    "    :return: a tuple containing Rutte texts and De Jonge texts respectively\n",
    "\n",
    "    \"\"\"\n",
    "    corpus = pd.DataFrame(columns=['words'])\n",
    "    nr_of_conferences = len(text_by_speaker[0])\n",
    "\n",
    "    # Create Dataframe with Word Counts \n",
    "    for i in tqdm(range(nr_of_conferences)):\n",
    "        full_conference_text = text_by_speaker[0][i]['text'] + text_by_speaker[1][i]['text']\n",
    "        words = [token.lemma_ for token in nlp(full_conference_text) if\n",
    "                 not (token.is_stop or token.is_punct or token.is_space)]\n",
    "\n",
    "        word_count = Counter(words)\n",
    "\n",
    "        new_words = list(set(word_count.keys()) - set(corpus['words']))\n",
    "        corpus = corpus.append(pd.DataFrame({'words': new_words}), ignore_index=True)\n",
    "\n",
    "        wordlist = []\n",
    "        for word in corpus['words']:\n",
    "            if word in word_count.keys():\n",
    "                wordlist.append(word_count[word])\n",
    "            else:\n",
    "                wordlist.append(0)\n",
    "\n",
    "        corpus[text_by_speaker[0][i]['date']] = wordlist\n",
    "\n",
    "    corpus.set_index('words', inplace=True)\n",
    "    corpus.fillna(0, inplace=True)\n",
    "\n",
    "    if create_csv:\n",
    "        corpus.to_csv('corpus.csv')\n",
    "\n",
    "    tf_idf = {k: [] for k in corpus.columns}\n",
    "\n",
    "    # Create Dataframe with Relative Word Frequencies \n",
    "    for index, row in tqdm(corpus.iterrows(), total=len(corpus)):\n",
    "        docs_with = np.count_nonzero(row)\n",
    "\n",
    "        for colname, count in row.items():\n",
    "            total_uniques = np.count_nonzero(corpus[colname])\n",
    "            value = (count / total_uniques) * math.log(len(corpus.columns) / docs_with)\n",
    "\n",
    "            tf_idf[colname].append(value)\n",
    "\n",
    "    tf_idf_df = pd.DataFrame.from_dict(tf_idf)\n",
    "    tf_idf_df.set_index(corpus.index, inplace=True)\n",
    "\n",
    "    if create_csv:\n",
    "        tf_idf_df.to_csv(('tf_idf.csv'))\n",
    "\n",
    "    return (corpus, tf_idf_df)\n",
    "\n",
    "    '''\n",
    "            total_uniques = len(word_list)\n",
    "            \n",
    "            # Get Word Frequency\n",
    "            word_frequency = list()\n",
    "            for word_tuple in word_list:\n",
    "                word, count = word_tuple\n",
    "                \n",
    "                docs_with = 0\n",
    "                # Check number of other text docs that contain the word\n",
    "                for i, conferences_list in enumerate(text_by_speaker):\n",
    "                    for j, conference in enumerate(conferences_list):\n",
    "                        if word in dict(conference['word_list']):\n",
    "                            docs_with += 1\n",
    "                        \n",
    "                count = count / total_uniques * math.log(nr_of_docs / docs_with)\n",
    "                word_frequency.append((word, count))\n",
    "    '''\n",
    "\n",
    "    \"\"\"\n",
    "    # works better than the Spacy sentence splitter\n",
    "    nr_of_docs = len(text_by_speaker[0]) + len(text_by_speaker[1])\n",
    "    \n",
    "    # Get Word Counts                 \n",
    "    for i, conferences_list in enumerate(text_by_speaker):\n",
    "        for j, conference in enumerate(conferences_list):\n",
    "            words = [token.lemma_ for token in nlp(conference['text']) if not (token.is_stop or token.is_punct or token.is_space)]\n",
    "    \n",
    "            word_count = Counter(words)\n",
    "\n",
    "            # Sorted word count                 \n",
    "            word_list = list(word_count.items())\n",
    "            word_list.sort(key=lambda item: item[1], reverse=True)\n",
    "\n",
    "            text_by_speaker[i][j]['word_list'] = word_list\n",
    "                     \n",
    "    # Get Relative Word Frequencies                 \n",
    "    for i, conferences_list in enumerate(text_by_speaker):\n",
    "        for j, conference in enumerate(conferences_list):       \n",
    "            \n",
    "            word_list = text_by_speaker[i][j]['word_list']\n",
    "                     \n",
    "            # length of word_list\n",
    "            total_uniques = len(word_list)\n",
    "            \n",
    "            # Get Word Frequency\n",
    "            word_frequency = list()\n",
    "            for word_tuple in word_list:\n",
    "                word, count = word_tuple\n",
    "                \n",
    "                docs_with = 0\n",
    "                # Check number of other text docs that contain the word\n",
    "                for i, conferences_list in enumerate(text_by_speaker):\n",
    "                    for j, conference in enumerate(conferences_list):\n",
    "                        if word in dict(conference['word_list']):\n",
    "                            docs_with += 1\n",
    "                        \n",
    "                count = count / total_uniques * math.log(nr_of_docs / docs_with)\n",
    "                word_frequency.append((word, count))\n",
    "            text_by_speaker[i][j]['word_frequency'] = word_frequency\n",
    "               \n",
    "    return text_by_speaker\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4804ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFERENCE_OUTPUT_FOLDER = '../input/conferences/'\n",
    "\n",
    "\n",
    "def _preprocess_all_conferences(tfidf=False, save_tfidf=False) -> tuple:\n",
    "    \"\"\"\n",
    "\n",
    "    Returns the preprocessed conference data\n",
    "\n",
    "    :return: a tuple containing Rutte texts and De Jonge texts respectively\n",
    "\n",
    "    \"\"\"\n",
    "    conference_paths = os.listdir(CONFERENCE_OUTPUT_FOLDER)\n",
    "    all_text_rutte, all_text_de_jonge = [], []\n",
    "\n",
    "    for conf_file_name in conference_paths:\n",
    "        with open(f\"{CONFERENCE_OUTPUT_FOLDER}/{conf_file_name}\", \"r\", encoding='utf-8') as f:\n",
    "            text_rutte, text_de_jonge = _preprocess_conference_data(f.readlines())\n",
    "            conf_date = conf_file_name.replace('.txt', '')\n",
    "            all_text_rutte.append({'date': conf_date, 'text': text_rutte})\n",
    "            all_text_de_jonge.append({'date': conf_date, 'text': text_de_jonge})\n",
    "\n",
    "    ### Disabled for now, should be an option run option (e.g. add boolean as parameter to this function?)\n",
    "    if False:\n",
    "        all_text_rutte, all_text_de_jonge = _get_sentence_length((all_text_rutte, all_text_de_jonge))\n",
    "\n",
    "    if tfidf:\n",
    "        all_text_rutte, all_text_de_jonge = _calculate_tfidf((all_text_rutte, all_text_de_jonge), save_tfidf=save_tfidf)\n",
    "\n",
    "    return all_text_rutte, all_text_de_jonge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded9eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = _preprocess_all_conferences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad3169",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, tf_idf = _calculate_tfidf(texts, create_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bd9919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tf_idf\n",
    "df = pd.DataFrame.from_dict(tf_idf)\n",
    "df.set_index(corpus.index[:5])\n",
    "\n",
    "for i in tqdm(tf_idf):\n",
    "    print('yes!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77b00f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dict.fromkeys(corpus.columns, [])\n",
    "print(test)\n",
    "test['2020-09-01'].append(1)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee0284",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = {'2020-09-01': [], '2020-09-18': [], '2020-09-28': [], '2020-10-13': []}\n",
    "print(test2)\n",
    "test2['2020-09-01'].append(1)\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50122714",
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = {k: [] for k in corpus.columns}\n",
    "print(test3)\n",
    "test3['2020-09-01'].append(1)\n",
    "print(test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_idf.keys())\n",
    "tf_idf['2020-09-18'] += [3]\n",
    "tf_idf['2020-09-18'].append(3)\n",
    "tf_idf['2020-10-13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a93d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted word count\n",
    "#word_list = list(word_count.items())\n",
    "#word_list.sort(key=lambda item: item[1], reverse=True)\n",
    "#words = [x[0] for x in word_list]\n",
    "#words_dict = {key:value for (key,value) in dictonary.items()}\n",
    "\n",
    "#temp_list = []\n",
    "#for word, count in word_list:\n",
    "#    if word not in corpus['words']:\n",
    "#        temp_list.append(word)\n",
    "\n",
    "\n",
    "#documents.append(conference['date'])\n",
    "#corpus[conference['date']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400567da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "my_df = pd.DataFrame()\n",
    "for i in ['a', 'b', 'c']:\n",
    "    my_df[i] = np.nan\n",
    "my_df\n",
    "for col in my_df.columns:\n",
    "    print(my_df[col])\n",
    "#print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7127f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa9708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "conference_paths = os.listdir('../input/toespraken/')\n",
    "conference_texts = []\n",
    "\n",
    "for conference in conference_paths:\n",
    "    with open(os.path.join('../input/toespraken/', conference), \"r\", encoding='utf-8') as f:\n",
    "        conference_texts.append({'filename': conference, 'text': f.readlines()})\n",
    "\n",
    "save_text = 'rutte'\n",
    "words = set()\n",
    "\n",
    "cleaned_conferences = []\n",
    "all_text_rutte, all_text_de_jonge = [], []\n",
    "for conference in conference_texts:\n",
    "    text_rutte, text_de_jonge = [], []\n",
    "    for line in conference['text'][3:]:\n",
    "        if save_text == 'rutte':\n",
    "            text_rutte.append(line)\n",
    "        if save_text == 'de jonge':\n",
    "            text_de_jonge.append(line)\n",
    "\n",
    "        if line.isupper():\n",
    "            if 'RUT' in line:\n",
    "                save_text = 'rutte'\n",
    "            elif 'DE JONGE' in line:\n",
    "                save_text = 'de jonge'\n",
    "\n",
    "            else:\n",
    "                words.add(line)\n",
    "                save_text = False\n",
    "\n",
    "    conference_date = conference['filename'][0:-4]\n",
    "\n",
    "    all_text_rutte.append({'date': conference_date, 'text': text_rutte})\n",
    "    all_text_de_jonge.append({'date': conference_date, 'text': text_de_jonge})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d414fe0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d3eec",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!py -m spacy download nl_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d1998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "CONFERENCE_OUTPUT_FOLDER = '../input/conferences/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62502f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_conference_data(conference_data: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Preprocesses the text of a single conference\n",
    "\n",
    "    :return: a tuple containing Rutte texts and De Jonge texts respectively per press conference\n",
    "    \"\"\"\n",
    "\n",
    "    # Starts with saving text for Rutte\n",
    "    save_text = 'rutte'\n",
    "    words = set()\n",
    "\n",
    "    # Only keep the sentences of Rutte and De Jonge\n",
    "    text_rutte, text_de_jonge = [], []\n",
    "    for line in conference_data:\n",
    "        if line.isupper():\n",
    "            if 'RUT' in line:\n",
    "                save_text = 'rutte'\n",
    "            elif 'DE JONGE' in line:\n",
    "                save_text = 'de jonge'\n",
    "            else:\n",
    "                words.add(line)\n",
    "                save_text = 'other'\n",
    "        else:\n",
    "            if save_text == 'rutte':\n",
    "                text_rutte.append(line)\n",
    "            if save_text == 'de jonge':\n",
    "                text_de_jonge.append(line)\n",
    "\n",
    "    return text_rutte, text_de_jonge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20eb765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_all_conferences() -> tuple:\n",
    "    \"\"\"\n",
    "\n",
    "    Returns the preprocessed conference data\n",
    "\n",
    "    :return: a tuple containing Rutte texts and De Jonge texts respectively\n",
    "\n",
    "    \"\"\"\n",
    "    conference_paths = os.listdir(CONFERENCE_OUTPUT_FOLDER)\n",
    "    all_text_rutte, all_text_de_jonge = [], []\n",
    "\n",
    "    for conf_file_name in conference_paths:\n",
    "        with open(f\"{CONFERENCE_OUTPUT_FOLDER}/{conf_file_name}\", \"r\", encoding='utf-8') as f:\n",
    "            text_rutte, text_de_jonge = _preprocess_conference_data(f.readlines())\n",
    "            conf_date = conf_file_name.replace('.txt', '')\n",
    "            all_text_rutte.append({'date': conf_date, 'text': text_rutte})\n",
    "            all_text_de_jonge.append({'date': conf_date, 'text': text_de_jonge})\n",
    "\n",
    "    return all_text_rutte, all_text_de_jonge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e97e4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_by_speaker = _preprocess_all_conferences()\n",
    "for i, conferences_list in enumerate(text_by_speaker):\n",
    "    for j, conference in enumerate(conferences_list):\n",
    "        # print(i, j, conference['date'])\n",
    "        full_conference_text = ''.join(conference['text']).replace(\"\\n\", \" \")\n",
    "        sentences = re.split(r'(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s(?<!\\w\\.\\w.\\s)', full_conference_text)\n",
    "        text_by_speaker[i][j]['number_of_sentences'] = len(sentences)\n",
    "        # print(text_by_speaker[i][j]['number_of_sentences'])\n",
    "print(text_by_speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fac865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "texts = _preprocess_all_conferences()\n",
    "single_conference_text = texts[0][0]['text']\n",
    "#print(single_conference_text)\n",
    "full_conference_text = 'xxx'.join(single_conference_text).replace(\"\\n\", \" \")\n",
    "#print(full_conference_text)\n",
    "\n",
    "# works better than the Spacy sentence splitter\n",
    "sentences = re.split(r'(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s(?<!\\w\\.\\w.\\s)', full_conference_text)\n",
    "#print(sentences)\n",
    "#for sentence in sentences:\n",
    "#    if len(sentence.split(' ')) < 10:\n",
    "#        print(sentence)\n",
    "\n",
    "#texts[0][0]['number_of_sentences'] = len(sentences)\n",
    "print(len(sentences))\n",
    "\n",
    "# i = 0\n",
    "processed_text = nlp(full_conference_text)\n",
    "sentences = [sentence.text for sentence in processed_text.sents]\n",
    "print(len(sentences))\n",
    "\n",
    "\n",
    "\n",
    "#processed_text = [text for text in tqdm(nlp.pipe(texts, n_process=-1, disable=[\"ner\", \"parser\"]), total=len(texts))]\n",
    "#lemmatized_text = [[word.lemma_.lower() for word in text if word.pos_ in POStags and not word.is_punct and not word.is_stop] for text in processed_text]\n",
    "#regexed_text = [[re.sub(r'\\W+', '', word) for word in text] for text in lemmatized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80715358",
   "metadata": {},
   "outputs": [],
   "source": [
    "tup = [('a', 1), ('b', 2)]\n",
    "for i, data in enumerate(tup):\n",
    "    print(i, data)\n",
    "\n",
    "b = (1, 3)\n",
    "c, d = b\n",
    "c\n",
    "l = tuple(list(tup))\n",
    "for i, data in enumerate(l):\n",
    "    print(l)\n",
    "[d, c]\n",
    "3535\n",
    "'a' in dict(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f67a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nlp(full_conference_text)\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68279c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "doc_number = 29\n",
    "docs_with = 1\n",
    "\n",
    "#full_conference_text.lower()\n",
    "\n",
    "words = [token.lemma_ for token in a if not (token.is_stop or token.is_punct or token.is_space)]\n",
    "words\n",
    "\n",
    "word_count = Counter(words)\n",
    "word_count.most_common(20)\n",
    "word_count\n",
    "\n",
    "# Get Sorted List\n",
    "word_list = list(word_count.items())\n",
    "word_list.sort(key=lambda item: item[1], reverse=True)\n",
    "print(word_list[:10])\n",
    "\n",
    "if 'omtt' in dict(word_list):\n",
    "    print('yes')\n",
    "\n",
    "# length of word_list\n",
    "total_uniques = len(word_list)\n",
    "\n",
    "word_frequency = list()\n",
    "for i in range(len(word_list)):\n",
    "    word, count = word_list[i]\n",
    "    count = count / total_uniques * math.log(doc_number / docs_with)\n",
    "    word_frequency.append((word, count))\n",
    "word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccdeae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}