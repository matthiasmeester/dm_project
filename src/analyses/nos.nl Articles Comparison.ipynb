{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c2a80b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09297e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "\n",
    "from itertools import chain\n",
    "import math\n",
    "\n",
    "from util.nos_articles import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import spacy\n",
    "# nlp = spacy.load(\"nl_core_news_sm\")\n",
    "nlp = spacy.load(\"nl_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b642f8",
   "metadata": {},
   "source": [
    "# Download Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "40a439c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dates = [x.strftime('%Y-%m-%d') for x in pd.date_range(start='2020-04-01', end='2020-12-31', freq='D')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c498afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if you want to download new articles\n",
    "# download_articles(dates=dates, max_articles=5000, refresh_articles=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f7d25",
   "metadata": {},
   "source": [
    "# Load Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "69b48c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed2de597ef249f9977ec224c9988699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "articles_dict = load_nos_texts(dates=dates)\n",
    "# articles_dict['2020-04-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfde08b",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "556fc16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_articles = list(chain(*articles_dict.values()))\n",
    "list_of_articles = [' '.join(x) for x in list_of_articles]\n",
    "\n",
    "# list_of_text = [item for sublist in list_of_articles for item in sublist]\n",
    "# print(len(list_of_text))\n",
    "# chunk_size =  int(len(list_of_text)/500)\n",
    "# print(chunk_size)\n",
    "# chunks = [list_of_text[x:x+chunk_size] for x in range(0, len(list_of_text), chunk_size)]\n",
    "# chunks[1]\n",
    "# string_chunks = [' '.join(x) for x in chunks]\n",
    "# len(list_of_text)\n",
    "# stringed_text = ' '.join(list_of_text)\n",
    "# stringed_text[:500]\n",
    "# len(string_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62c1b59",
   "metadata": {},
   "source": [
    "# Calculate Average Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb996eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.202796550050405"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_sentence_length = get_average_sentence_length(stringed_text)\n",
    "average_sentence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727cfb3e",
   "metadata": {},
   "source": [
    "# Create Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad914276",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_PATH = '../output/NOS_corpus_5000_articles_without_stopwords.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63effe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(full_text: list, save=False, stopwords=False) -> tuple:\n",
    "    nlp.max_length = 1500000\n",
    "    processed_texts = [text for text in tqdm(nlp.pipe(full_text), total=len(full_text))]\n",
    "    \n",
    "    lemmatized_texts = [[word.lemma_ for word in processed_text if not (word.is_punct or word.is_space or (stopwords and word.is_stop))] \\\n",
    "                        for processed_text in tqdm(processed_texts, total=len(processed_texts))]\n",
    "\n",
    "    flattened_lemmatized_texts = [item for sublist in lemmatized_texts for item in sublist]\n",
    "    word_count = Counter(flattened_lemmatized_texts)\n",
    "    \n",
    "    # Create Dataframe with Word Counts\n",
    "    corpus = pd.DataFrame(columns=['word'])\n",
    "    \n",
    "    # Allows you to differentiate between subsets by creating columns for each of them\n",
    "    new_word = list(set(word_count.keys()) - set(corpus['word']))\n",
    "    corpus = corpus.append(pd.DataFrame({'word': new_word}), ignore_index=True)\n",
    "\n",
    "    wordlist = []\n",
    "    for word in corpus['word']:\n",
    "        if word in word_count.keys():\n",
    "            wordlist.append(word_count[word])\n",
    "        else:\n",
    "            wordlist.append(0)\n",
    "\n",
    "    corpus['nos'] = wordlist\n",
    "    corpus.set_index('word', inplace=True)\n",
    "    corpus.fillna(0, inplace=True)\n",
    "    corpus.sort_values(by='nos', ascending=False, inplace=True)\n",
    "    \n",
    "    if save:\n",
    "        corpus.to_csv(CORPUS_PATH)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c7ee1cfc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153f6469c4c04e6ca5280b3c86b94162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e730e9dbb7244d1983d3165b357e6555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>jeugdig</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winnen</th>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>katinka</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>veiligheidsbril</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tijdstraf</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wvggz</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ryanair</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>werk-voucher</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ruit</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overgave</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53323 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 nos\n",
       "word                \n",
       "jeugdig            1\n",
       "winnen           187\n",
       "katinka            2\n",
       "veiligheidsbril    2\n",
       "tijdstraf          1\n",
       "...              ...\n",
       "wvggz              1\n",
       "ryanair           26\n",
       "werk-voucher       1\n",
       "ruit              11\n",
       "overgave           3\n",
       "\n",
       "[53323 rows x 1 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# string_chunks[1][:5000].split(' ')\n",
    "corpus = create_corpus(list_of_articles, save=False)\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8047ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document has 855629 words, with 53323 unique values.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Document has {corpus['nos'].sum()} words, with {corpus['nos'].count()} unique values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbcee9b",
   "metadata": {},
   "source": [
    "# Create tf & tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f9caf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_CSV_PATH = '../output/NOS_tf_idf_5000_articles_without_stopwords.csv'\n",
    "TF_CSV_PATH = '../output/NOS_tf_5000_articles_without_stopwords.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d4478f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('../output/NOS_corpus_5000_articles_without_stopwords.csv')\n",
    "corpus.set_index('word', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "edba9a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(corpus, specified_path, include_idf=False, save=False):\n",
    "    # Create Dataframe with Relative Word Frequencies\n",
    "    tf_idf = {k: [] for k in corpus.columns}\n",
    "    for index, row in tqdm(corpus.iterrows(), total=len(corpus)):\n",
    "        docs_with = np.count_nonzero(row)\n",
    "\n",
    "        for colname, count in row.items():\n",
    "            \n",
    "            total_uniques = np.count_nonzero(corpus[colname])\n",
    "            tf = count / total_uniques\n",
    "\n",
    "            if include_idf:\n",
    "                idf = math.log(len(corpus.columns) / docs_with)\n",
    "                tf *= idf\n",
    "\n",
    "            tf_idf[colname].append(tf)\n",
    "\n",
    "    tf_idf_df = pd.DataFrame.from_dict(tf_idf)\n",
    "    tf_idf_df.set_index(corpus.index, inplace=True)\n",
    "    tf_idf_df.sort_values(by='nos', ascending=False, inplace=True)\n",
    "    \n",
    "    if save:\n",
    "        if include_idf:\n",
    "            tf_idf_df.to_csv(TF_IDF_CSV_PATH)\n",
    "            print(f\"Saved TF-IDF DataFrame to '{specified_path}'\")\n",
    "        else:\n",
    "            tf_idf_df.to_csv(TF_CSV_PATH)\n",
    "            print(f\"Saved TF DataFrame to '{specified_path}'\")\n",
    "            \n",
    "    return tf_idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02e50d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786d39e82a704e969f7496f4147fe37e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TF DataFrame to '../output/NOS_tf_5000_articles_without_stopwords.csv'\n"
     ]
    }
   ],
   "source": [
    "tf_df = calculate_tfidf(corpus=corpus, include_idf=False, specified_path=TF_CSV_PATH, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5409c713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e2f569fbd947d4bb7f25b0604d407c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TF-IDF DataFrame to '../output/NOS_tf_idf_5000_articles_without_stopwords.csv'\n"
     ]
    }
   ],
   "source": [
    "tf_idf_df = calculate_tfidf(corpus=corpus, include_idf=True, specified_path=TF_IDF_CSV_PATH, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954441d6",
   "metadata": {},
   "source": [
    "# Basic Word Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e050377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
